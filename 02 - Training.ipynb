{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "561451b2",
   "metadata": {},
   "source": [
    "# 02 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e311c165",
   "metadata": {},
   "source": [
    "## A - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cba4394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AnacondaDestination\\envs\\metabodgd\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from metaboDGD.util import data, train\n",
    "from metaboDGD.src import model\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "import umap\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac99ca",
   "metadata": {},
   "source": [
    "## B - Retrieve Dataframe and Cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a58b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'outputs/'\n",
    "df_fname = 'CombinedDataset_CAMP_Normal.csv'\n",
    "df_tumor_fname = 'CombinedDataset_CAMP_Tumor.csv'\n",
    "cohorts_fname = 'cohorts.pkl'\n",
    "cohorts_tumor_fname = 'cohorts_tumor.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d6560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dir + df_fname)\n",
    "df.set_index('Unnamed: 0', inplace=True)\n",
    "df.index.name = None\n",
    "df_lbls = df.loc['cohort'].to_list()\n",
    "df_log = df.T.drop(columns=['cohort']).astype('float64')\n",
    "\n",
    "df_tumor = pd.read_csv(dir + df_tumor_fname)\n",
    "df_tumor.set_index('Unnamed: 0', inplace=True)\n",
    "df_tumor.index.name = None\n",
    "df_tumor_lbls = df_tumor.loc['cohort'].to_list()\n",
    "df_tumor_log = df_tumor.T.drop(columns=['cohort']).astype('float64')\n",
    "\n",
    "\n",
    "# df_exp = pd.read_csv(dir + df_exp_fname)\n",
    "# df_exp.set_index('Unnamed: 0', inplace=True)\n",
    "# df_exp.index.name = None\n",
    "\n",
    "\n",
    "f = open(dir + cohorts_fname, 'rb')\n",
    "cohorts = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "ft = open(dir + cohorts_tumor_fname, 'rb')\n",
    "cohorts_tumor = pickle.load(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb27bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Apply SMOTE so all classes are of size 47\n",
    "# sm = SMOTE(random_state=100, k_neighbors=2)\n",
    "# np_log_smote, df_lbls_smote = sm.fit_resample(df_log.to_numpy(), np.array(df_lbls))\n",
    "\n",
    "# ## Update the sample_list for data loaders later\n",
    "# complete_list = []\n",
    "# for c in cohorts:\n",
    "#     sample_list = cohorts[c]['sample_list']\n",
    "\n",
    "#     if len(sample_list) < 47:\n",
    "#         smote_sample_list = [c + '_' + str(x+1) for x in range(len(sample_list), 47)]\n",
    "#         cohorts[c]['sample_list'] += smote_sample_list\n",
    "#     complete_list += cohorts[c]['sample_list']\n",
    "        \n",
    "# df_log_smote = pd.DataFrame(np_log_smote, index=complete_list, columns=df_log.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17220cca",
   "metadata": {},
   "source": [
    "## C - Preparing TrainLoader and DGD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b6eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "test_dict  = {}\n",
    "\n",
    "train_lbls = []\n",
    "test_lbls  = []\n",
    "\n",
    "plot_counts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611ec14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRCA1: 37\n",
      "ccRCC3: 37\n",
      "ccRCC4: 19\n",
      "COAD: 31\n",
      "GBM: 4\n",
      "HurthleCC: 2\n",
      "PDAC: 9\n",
      "PRAD: 36\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = data.create_dataloaders(\n",
    "    cohorts=cohorts,\n",
    "    df=df_log,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9a45950",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgd_model = model.MetaboDGD(\n",
    "    latent_dim=5,\n",
    "    output_dim=df_log.shape[1],\n",
    "    dec_hidden_layers_dim=[500, 1500],\n",
    "    dec_output_prediction_type='mean',\n",
    "    dec_output_activation_type='softplus',\n",
    "    n_comp=8,\n",
    "    cm_type='diagonal'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a44c93ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb_prior_sample = dgd_model.gmm.means_prior['dist'].sample(10000)\n",
    "# log_prob = dgd_model.gmm.means_prior['dist'].log_prob(sb_prior_sample).numpy()\n",
    "# true_prob = np.exp(log_prob)\n",
    "# plt.hist(log_prob, bins=50, density=True, edgecolor='black')\n",
    "# plt.hist(sb_prior_sample[:,0], bins=50, density=True, edgecolor='black')\n",
    "# plt.hist(sb_prior_sample[:,1], bins=50, density=True, edgecolor='black')\n",
    "# plt.hist2d(sb_prior_sample[:,0], sb_prior_sample[:,1], bins=50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0575834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "dgd_model, train_rep, test_rep, history, cm = train.train_dgd(\n",
    "    dgd_model=dgd_model,\n",
    "    train_loader=train_loader,\n",
    "    validation_loader=test_loader,\n",
    "    n_epochs=200,\n",
    "    lr_schedule_epochs=None,\n",
    "    lr_schedule=[1e-4, 1e-3, 1e-2],\n",
    "    optim_betas=[0.5, 0.7],\n",
    "    wd=1e-5,\n",
    ")\n",
    "\n",
    "# train.get_history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7999526",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader, rep_layer_init = dgd_model.get_representations(df_tumor_log, df_tumor_lbls, df_tumor_log.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3acdd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8, 1915])\n",
      "torch.Size([16, 8, 1915])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     45\u001b[39m             recon_loss = dgd.dec.normal_layer.loss(obs_counts, pred_means)\n\u001b[32m     46\u001b[39m             \u001b[38;5;66;03m# print(recon_loss.shape)\u001b[39;00m\n\u001b[32m     47\u001b[39m             \u001b[38;5;66;03m# recon_loss_sum = recon_loss.sum(-1).clone()\u001b[39;00m\n\u001b[32m     48\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     62\u001b[39m \n\u001b[32m     63\u001b[39m         \u001b[38;5;66;03m# return best_reps\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43mselect_best_rep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdgd_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep_layer_init\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mselect_best_rep\u001b[39m\u001b[34m(dgd, data_loader, rep_layer)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(obs_counts.shape)\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(pred_means.shape)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m recon_loss = \u001b[43mdgd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdec\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_means\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alfonso Miguel Cruz\\Desktop\\scDGD\\metaboDGD\\metaboDGD\\src\\decoder.py:243\u001b[39m, in \u001b[36mHurdleLogNormalLayer.loss\u001b[39m\u001b[34m(self, x, y)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.any(torch.isnan(nll_zr)) \u001b[38;5;129;01mor\u001b[39;00m torch.any(torch.isnan(nll_nz)):\n\u001b[32m    241\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNAN FOUND! (NLL_ZR, NLL_NZ)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnll_nz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnll_zr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (8) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "def select_best_rep(dgd, data_loader, rep_layer):\n",
    "        n_samples = len(data_loader.dataset)\n",
    "\n",
    "        n_comp = dgd.gmm.n_comp\n",
    "\n",
    "        dim = dgd.gmm.dim\n",
    "        \n",
    "        n_metabolites = dgd.dec.nn[-1].out_features\n",
    "\n",
    "        best_reps = torch.empty(size=(n_samples, dim))\n",
    "\n",
    "        for x, i in data_loader:\n",
    "            n_batch_size = len(i)\n",
    "\n",
    "            # n_samples * n_comp * n_rep_per_comp , dim\n",
    "            z_all = rep_layer()\n",
    "\n",
    "            # n_samples , n_rep_per_comp , n_comp, dim\n",
    "            z_3d = z_all.view(\n",
    "                n_samples,\n",
    "                n_comp,\n",
    "                dim\n",
    "            )[i]\n",
    "\n",
    "            # n_samples_in_batch * n_comp * n_rep_per_comp , dim\n",
    "            z = z_3d.view(\n",
    "                n_batch_size * n_comp,\n",
    "                dim\n",
    "            )\n",
    "\n",
    "            # n_samples_in_batch * n_comp * n_rep_per_comp , n_genes\n",
    "            dec_out = dgd.dec(z)\n",
    "\n",
    "            # n_samples_in_batch , 1 , n_comp, n_genes\n",
    "            obs_counts = x.unsqueeze(1).expand(-1, n_comp ,-1)\n",
    "\n",
    "            # n_samples_in_batch , 1 , 1 , 1\n",
    "            \n",
    "            # n_samples_in_batch , n_rep_per_comp , n_comp, n_genes\n",
    "            # n_samples_in_batch, n_comp, n_metabolites\n",
    "            pred_means = dec_out.view(n_batch_size, n_comp, n_metabolites)\n",
    "\n",
    "            print(obs_counts.shape)\n",
    "            print(pred_means.shape)\n",
    "            recon_loss = dgd.dec.normal_layer.loss(obs_counts, pred_means)\n",
    "            # print(recon_loss.shape)\n",
    "            # recon_loss_sum = recon_loss.sum(-1).clone()\n",
    "\n",
    "            ## Reshape recon_loss_sum to (n_batch_size * n_comp)\n",
    "\n",
    "            # gmm_loss = self.gmm(z).clone()\n",
    "\n",
    "            # total_loss = recon_loss_sum_reshaped + gmm_loss\n",
    "\n",
    "            ## Reshape total_loss to (n_batch_size * n_comp)\n",
    "\n",
    "            ## best_rep_per_sample = torch.argmin(total_loss_reshaped, dim=1).squeeze(-1)\n",
    "\n",
    "            ## Reshape z to (n_batch_size, n_comp, dim)[range(n_batch_size), best_rep_per_sample]\n",
    "\n",
    "            # best_reps[i] = rep\n",
    "\n",
    "        # return best_reps\n",
    "\n",
    "\n",
    "select_best_rep(dgd_model, data_loader, rep_layer_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.repeat_interleave(dgd_model.gmm.means.clone().detach().unsqueeze(0),\n",
    "                        45,\n",
    "                        dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d50dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_norm = np.round(np.nan_to_num(cm / np.sum(cm, axis=0) * 100), decimals=1)\n",
    "np.sum(cm, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff747ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_lbls = list(cohorts.keys())\n",
    "sample_num_lbls = [len(cohorts[c]['sample_list']) for c in cohorts.keys()]\n",
    "plt.rcParams[\"font.family\"] = 'Arial'\n",
    "# plt.rcParams[\"font.family\"] = 'sans-serif'\n",
    "\n",
    "zr_mask = np.where(cm_norm > 0, False, True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "\n",
    "sns.heatmap(cm_norm, cmap='Blues',\n",
    "            annot=True,\n",
    "            fmt='g',\n",
    "            xticklabels=np.sum(cm, axis=0),\n",
    "            yticklabels=class_lbls,\n",
    "            linewidths=0.1,\n",
    "            mask=zr_mask,\n",
    "            linecolor='black',\n",
    "            ax=ax,\n",
    "            cbar_kws={\n",
    "                'pad': 0.15,\n",
    "                'shrink': 0.375,\n",
    "                'aspect': 5,\n",
    "                'anchor': (0.0, 1.0)\n",
    "            })\n",
    "\n",
    "ax_y = ax.twinx()\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "ax.tick_params(axis='both', length=0)\n",
    "\n",
    "ax_y.set_yticks(ax.get_yticks())\n",
    "ax_y.set_ylim(ax.get_ylim())\n",
    "ax_y.set_yticklabels(sample_num_lbls)\n",
    "ax_y.tick_params(axis='y', length=0)\n",
    "\n",
    "ax.set_xlabel('Samples per component', fontsize=14, labelpad=15)\n",
    "ax_y.set_ylabel('Samples per tissue' , fontsize=14, labelpad=30, rotation=270)\n",
    "\n",
    "ax.tick_params(  axis='y', direction='out', pad=5)\n",
    "ax_y.tick_params(axis='y', direction='out', pad=5)\n",
    "\n",
    "ax.set_title('Gaussian Components', pad=15, fontsize=14)\n",
    "\n",
    "# fig.subplots_adjust(bottom=0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.log(10) * -2\n",
    "0.2 * .125 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153ca99c",
   "metadata": {},
   "source": [
    "# OTHER CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1144a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dgd_model.gmm.weights.detach())\n",
    "print(dgd_model.gmm.get_mixture_probs().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02179bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "pca_fit = pca.fit_transform(df_log.to_numpy())\n",
    "for (i,j) in [(0,47), (47,94), (94,118), (118,157), (157,163), (163,166), (166,178), (178,224)]:\n",
    "    print(np.var(pca_fit[i:j], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59379589",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "[2.32078454,15.44278658,  0.16024287, 0.50076828, 1.44459359],\n",
    "[5.76099106,  2.9719728,  1.77597046, 0.45738981, 1.58652234],\n",
    "[11.66611275,1.32510036, 10.29634677, 0.31121013, 1.57775395],\n",
    "[0.27430105, 0.31315221,  0.38402532, 0.03392115, 0.17598611],\n",
    "[0.16735727, 0.30873464,  0.02512788, 3.94248676, 0.10560584],\n",
    "[2.38964571, 1.12885473,  0.11152993, 0.04679803, 0.59536716],\n",
    "[1.11657805, 2.01189821,  0.58198513, 0.28459389, 5.12391001],\n",
    "[2.41024798, 1.63209868,  0.36540129, 0.19185093, 1.94765308],\n",
    "], dtype=np.float64)\n",
    "x = x.flatten()\n",
    "\n",
    "plt.hist(x)\n",
    "plt.axvline(x=np.mean(x), c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c015795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_fit = pca.fit_transform(df_log.to_numpy())\n",
    "# pca_fit = pca.fit_transform(np.exp2(df.T.to_numpy()[:,:-1].astype(np.float64)))\n",
    "# pca_fit = pca.fit_transform(train_rep.z.detach().numpy())\n",
    "# pca_fit = pca.fit_transform(test_rep.z.detach().numpy())\n",
    "\n",
    "# um = umap.UMAP(random_state=100)\n",
    "# pca_fit = um.fit_transform(df.T.to_numpy()[:, :-1])\n",
    "# pca_fit = um.fit_transform(df_exp_smote.to_numpy())\n",
    "# pca_fit = um.fit_transform(train_rep.z.detach().numpy())\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "### ALL \n",
    "ax.scatter(pca_fit[0:47,0]   , pca_fit[0:47,1]   , label='BRCA1'    )\n",
    "ax.scatter(pca_fit[47:94,0]  , pca_fit[47:94,1]  , label='COAD'     )\n",
    "ax.scatter(pca_fit[94:118,0]  , pca_fit[94:118,1]  , label='ccRCC3' )\n",
    "ax.scatter(pca_fit[118:157,0] , pca_fit[118:157,1] , label='ccRCC4' )\n",
    "ax.scatter(pca_fit[157:163,0], pca_fit[157:163,1], label='GBM'      )\n",
    "ax.scatter(pca_fit[163:166,0], pca_fit[163:166,1], label='HurthleCC')\n",
    "ax.scatter(pca_fit[166:178,0], pca_fit[166:178,1], label='PDAC'     )\n",
    "ax.scatter(pca_fit[178:224,0], pca_fit[178:224,1], label='PRAD'     )\n",
    "# ax.set_title('PCA - ALL')\n",
    "\n",
    "### TRAIN_REP\n",
    "# ax.scatter(pca_fit[0:37,0]   , pca_fit[0:37,1]   , label='BRCA1'    )\n",
    "# ax.scatter(pca_fit[37:74,0]  , pca_fit[37:74,1]  , label='COAD'     )\n",
    "# ax.scatter(pca_fit[74:93,0]  , pca_fit[74:93,1]  , label='ccRCC3'   )\n",
    "# ax.scatter(pca_fit[93:124,0] , pca_fit[93:124,1] , label='ccRCC4'   )\n",
    "# ax.scatter(pca_fit[124:128,0], pca_fit[124:128,1], label='GBM'      )\n",
    "# ax.scatter(pca_fit[128:130,0], pca_fit[128:130,1], label='HurthleCC')\n",
    "# ax.scatter(pca_fit[130:139,0], pca_fit[130:139,1], label='PDAC'     )\n",
    "# ax.scatter(pca_fit[139:175,0], pca_fit[139:175,1], label='PRAD'     )\n",
    "# ax.set_title('PCA - TRAIN_REP')\n",
    "\n",
    "### TEST_REP\n",
    "# ax.scatter(pca_fit[0:10,0]   , pca_fit[0:10,1]   , label='BRCA1' )\n",
    "# ax.scatter(pca_fit[10:20,0]  , pca_fit[10:20,1]  , label='COAD'  )\n",
    "# ax.scatter(pca_fit[20:25,0]  , pca_fit[20:25,1]  , label='ccRCC3')\n",
    "# ax.scatter(pca_fit[25:33,0] , pca_fit[25:33,1] , label='ccRCC4'  )\n",
    "# ax.scatter(pca_fit[33:35,0], pca_fit[33:35,1], label='GBM'       )\n",
    "# ax.scatter(pca_fit[35:36,0], pca_fit[35:36,1], label='HurthleCC' )\n",
    "# ax.scatter(pca_fit[36:39,0], pca_fit[36:39,1], label='PDAC'      )\n",
    "# ax.scatter(pca_fit[39:49,0], pca_fit[39:49,1], label='PRAD'      )\n",
    "# ax.set_title('PCA - TEST_REP')\n",
    "\n",
    "\n",
    "### SMOTE - TEST_REP\n",
    "# ax.scatter(pca_fit[0:10,0]   , pca_fit[0:10,1]   , label='BRCA1' )\n",
    "# ax.scatter(pca_fit[10:20,0]  , pca_fit[10:20,1]  , label='COAD'  )\n",
    "# ax.scatter(pca_fit[20:30,0]  , pca_fit[20:30,1]  , label='ccRCC3')\n",
    "# ax.scatter(pca_fit[30:40,0] , pca_fit[30:40,1] , label='ccRCC4'  )\n",
    "# ax.scatter(pca_fit[40:50,0], pca_fit[40:50,1], label='GBM'       )\n",
    "# ax.scatter(pca_fit[50:222,0], pca_fit[50:60,1], label='HurthleCC')\n",
    "# ax.scatter(pca_fit[60:70,0], pca_fit[60:70,1], label='PDAC'      )\n",
    "# ax.scatter(pca_fit[70:80,0], pca_fit[70:80,1], label='PRAD'      )\n",
    "\n",
    "### SMOTE - TRAIN_REP\n",
    "# ax.scatter(pca_fit[0:37,0]   , pca_fit[0:37,1]     , label='BRCA1'    )\n",
    "# ax.scatter(pca_fit[37:74,0]  , pca_fit[37:74,1]    , label='COAD'     )\n",
    "# ax.scatter(pca_fit[74:111,0]  , pca_fit[74:111,1]  , label='ccRCC3'   )\n",
    "# ax.scatter(pca_fit[111:148,0] , pca_fit[111:148,1] , label='ccRCC4'   )\n",
    "# ax.scatter(pca_fit[148:185,0], pca_fit[148:185,1]  , label='GBM'      )\n",
    "# ax.scatter(pca_fit[185:222,0], pca_fit[185:222,1]  , label='HurthleCC')\n",
    "# ax.scatter(pca_fit[222:259,0], pca_fit[222:259,1]  , label='PDAC'     )\n",
    "# ax.scatter(pca_fit[259:296,0], pca_fit[259:296,1]  , label='PRAD'     )\n",
    "\n",
    "### SMOTE - ALL\n",
    "# ax.scatter(pca_fit[0:47,0]   , pca_fit[0:47,1]   , label='BRCA1'    )\n",
    "# ax.scatter(pca_fit[47:94,0]  , pca_fit[47:94,1]  , label='COAD'     )\n",
    "# ax.scatter(pca_fit[94:141,0]  , pca_fit[94:141,1]  , label='ccRCC3' )\n",
    "# ax.scatter(pca_fit[141:188,0] , pca_fit[141:188,1] , label='ccRCC4' )\n",
    "# ax.scatter(pca_fit[188:235,0], pca_fit[188:235,1], label='GBM'      )\n",
    "# ax.scatter(pca_fit[235:282,0], pca_fit[235:282,1], label='HurthleCC')\n",
    "# ax.scatter(pca_fit[282:329,0], pca_fit[282:329,1], label='PDAC'     )\n",
    "# ax.scatter(pca_fit[329:376,0], pca_fit[329:376,1], label='PRAD'     )\n",
    "\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({(pca.explained_variance_ratio_[0] * 100):.2f}%)', labelpad=10)\n",
    "ax.set_ylabel(f'PC2 ({(pca.explained_variance_ratio_[1] * 100):.2f}%)')\n",
    "# ax.set_xlabel('UMAP D1')\n",
    "# ax.set_ylabel('UMAP D2')\n",
    "ax.legend(bbox_to_anchor=(1.275, 1),loc='upper right', fancybox=False, framealpha=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b1893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# lbls_all = df.T['cohort'].to_list()\n",
    "# # le = LabelEncoder()\n",
    "# # lbls_all_id = le.fit_transform(lbls_all)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(7,5))\n",
    "# pca = PCA(n_components=2)\n",
    "# pca_fit = pca.fit_transform(df_log.to_numpy())\n",
    "# ax.scatter(pca_fit[:, 0], pca_fit[:, 1], c=lbls_all_id, cmap='tab10')\n",
    "# # ax.set_title('PCA - ALL')\n",
    "# ax.set_xlabel(f'PC1: {(pca.explained_variance_ratio_[0] * 100):.2f}%')\n",
    "# ax.set_ylabel(f'PC2: {(pca.explained_variance_ratio_[1] * 100):.2f}%')\n",
    "\n",
    "# ax.legend(bbox_to_anchor=(1.30, 1),loc='upper right', fancybox=False, framealpha=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be658c56",
   "metadata": {},
   "source": [
    "# WANDB Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fbd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 873f16d32cf5f8595722bf04c6ead0f9415ad6af\n",
    "import random\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Start a new wandb run to track this script.\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"alfonso_cruz-de-la-salle-university\",\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"my-first-project\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"architecture\": \"CNN\",\n",
    "        \"dataset\": \"CIFAR-100\",\n",
    "        \"epochs\": 10,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Simulate training.\n",
    "epochs = 10\n",
    "offset = random.random() / 5\n",
    "for epoch in range(2, epochs):\n",
    "    acc = 1 - 2**-epoch - random.random() / epoch - offset\n",
    "    loss = 2**-epoch + random.random() / epoch + offset\n",
    "\n",
    "    # Log metrics to wandb.\n",
    "    run.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# Finish the run and upload any remaining data.\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fda8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'grid',\n",
    "    'metric': {\n",
    "        'name': 'cluster_acc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'paramters': {\n",
    "        'latent_dim': {\n",
    "            'values': [5, 10, 20, 30],\n",
    "        },\n",
    "        'dirichlet_alpha': {\n",
    "            'values': [0.5, 1, 3, 5],\n",
    "        },\n",
    "        'softball_radius': {\n",
    "            'values': [1, 5, 10],\n",
    "        },\n",
    "        'softball_sharpness': {\n",
    "            'values': [1, 5, 10],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "sweep_id = wandb.sweep(sweep_config, project='TODO')\n",
    "wandb.agent(sweep_id, train, count=5)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd46e50",
   "metadata": {},
   "source": [
    "## WITH SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e32fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cohorts = cohorts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5792c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X = df_exp.to_numpy()\n",
    "y = np.array(df_lbls)\n",
    "\n",
    "sm = SMOTE(random_state=100, k_neighbors=2)\n",
    "X_new, y_new = sm.fit_resample(X, y)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "res = pca.fit_transform(X_new)\n",
    "\n",
    "sum = 0\n",
    "colors = {\n",
    "    'BRCA1': 'red',\n",
    "    'ccRCC3': 'orange',\n",
    "    'ccRCC4': 'yellow',\n",
    "    'COAD': 'green',\n",
    "    'GBM': 'blue',\n",
    "    'HurthleCC': 'purple',\n",
    "    'PDAC': 'brown',\n",
    "    'PRAD': 'black',\n",
    "}\n",
    "\n",
    "for i in np.unique(y_new).tolist():\n",
    "    plt.scatter(res[sum:sum+47, 0], res[sum:sum+47, 1], c=colors[i])\n",
    "    sum += 47\n",
    "\n",
    "plt.xlabel(pca.explained_variance_ratio_[0] * 100)\n",
    "plt.ylabel(pca.explained_variance_ratio_[1] * 100)\n",
    "plt.title(\"PCA after SMOTE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metabodgd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
