import torch
import torch.nn as nn
import torch.distributions as D

class GaussianLayer (nn.Module):
    """
    Class implementing the output layer and the
    reconstruction loss functions.

    Parameters
    ----------
    output_dim : int
        Dimension of the decoder output.

    output_prediction_type : str, default="mean"
        Prediction type of the decoder.

    output_activation_type : str, default="leakyrelu"
        Activation function used in the output layer.
    """

    def __init__(
        self,
        output_dim,
        output_prediction_type="mean",
        output_activation_type="leakyrelu",
    ):
        """
        Initializes the output layer object.
        """

        super().__init__()
        
        # Standard deviations
        self.std = nn.Parameter(
            torch.full(fill_value=3.0, size=(1, output_dim), dtype=torch.float64),
            requires_grad=True
        )

        # Output prediction type
        self.output_prediction_type = output_prediction_type

        # Output activation type
        self.output_activation_type = output_activation_type


        if output_activation_type == 'leakyrelu':
            self.activation_layer = nn.LeakyReLU()
        elif output_activation_type == 'softplus':
            self.activation_layer = nn.Softplus()
        elif output_activation_type == 'prelu':
            self.activation_layer = nn.PReLU()
        else:
            self.activation_layer = None


    
    def forward(self, x):
        """
        Apply the activation layer to the previous output of the decoder.

        Parameters
        ----------
        x : `torch.Tensor`
        Intermediate output of the decoder.


        Returns
        -------
        y : `torch.Tensor`
        Final output of the decoder.
        """
        
        y = self.activation_layer(x)
        return y


    def loss(self, x, y):
        """
        Compute the reconstruction loss of the DGD model.

        Parameters
        ----------
        x : `torch.Tensor`
            A tensor of metabolite abundances of a sample.

        y : `torch.Tensor`
            A tensor of metabolite abundances generated by
            the decoder, to be used as the mean of the
            independent normal distributions.

        Returns
        -------
        recon_loss : `torch.Tensor`
            A tensor of negative-log probabilities of observing
            each abundance of x in every normal distribution. 
        """

        # Epsilon value to prevent 0.0
        eps = torch.full(fill_value=1e-5, size=x.shape)

        # Create normal distributions using decoder output as mean
        normal = D.Normal(loc=y, scale=self.std)
        
        # Get the negative-log probability
        recon_loss = -normal.log_prob(x+eps)

        # Return the reconstruction loss
        return recon_loss


class Decoder(nn.Module):
    """
    Class implementing the decoder component of the
    deep generative decoder model.

    Parameters
    ----------
    latent_layer_dim : int
        Dimension of the decoder input.

    output_dim : int
        Dimension of the decoder output.

    hidden_layer_dim : list, default=[100, 100]
        Dimensions of the decoder hidden layers.

    output_prediction_type : str, default="mean"
        Prediction type of the decoder.

    output_activation_type : str, default="leakyrelu"
        Activation function used in the output layer.
    """
    def __init__(
        self,
        latent_layer_dim,
        output_layer_dim,
        hidden_layer_dim=[100, 100],
        output_prediction_type="mean",
        output_activation_type="leakyrelu"
    ):
        super().__init__()

        # List of layers of the decoder network
        self.nn = nn.ModuleList()

        n_hidden_layers = len(hidden_layer_dim) + 1
        for i in range(n_hidden_layers):
            if i == 0:
                self.nn.append(nn.Linear(latent_layer_dim, hidden_layer_dim[i]))
                self.nn.append(nn.PReLU())
            elif i == n_hidden_layers - 1:
                self.nn.append(nn.Linear(hidden_layer_dim[-1], output_layer_dim))
            else:
                self.nn.append(nn.Linear(hidden_layer_dim[i-1], hidden_layer_dim[i]))
                self.nn.append(nn.PReLU())

        # Output layer of the network
        self.normal_layer = GaussianLayer(
            output_dim=output_layer_dim,
            output_prediction_type=output_prediction_type,
            output_activation_type=output_activation_type
        )
    
    
    def forward(self, z):
        """
        ##################################################
        Apply the activation layer to the previous output
        of the decoder.

        Parameters
        ----------
        z : `torch.Tensor`
            A tensor containing the latent representations
            to pass through the decoder.


        Returns
        -------
        y : `torch.Tensor`
            A tensor containing the decoder outputs from
            the given latent representations.
        """

        for i in range(len(self.nn)):
            z = self.nn[i](z)

        y = self.normal_layer(z)
        return y


    @classmethod
    def load(cls):
        pass